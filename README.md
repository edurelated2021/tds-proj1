# tds-proj1
For a selected city, perform analytics on it's Github users and repositories 

Methodology for collecting the data:  
1. The first thought was to use the Github users api. As part of my workflow, I did a trial run (using just plain browser GET request with Pretty Print option turned on in Chrome) to just get a feel of the data. In my case for the city of Chennai, I realized there are 423 users with a follower count greater than 50.  
2. However, I realized that just the users api will not get me the data required in the project. We need to go one level deep, so to speak, to retreieve the user details (data such as their email id, location, repositories they are following etc.). In order to do so, one would need to make the first api call (which in my case shows there are 423 users meeting the criteria in the city of Chennai), followed by individual api calls for each of the individual users (which makes it 423 additional calls - assuming there are no server side issues where I need to rerun the scraping program).
3. I read up the Github api limits and there is a cap of 60 requests/hour. So some back of the envelop calculation shows that it will take more than 4 hours to retrieve the user data alone (fetch first 60 user details, wait for another hour, rerun from where you left). That looked doable but very inefficient. I cross verified the Discourse forum and the course instructor has specifically mentioned that the project parameters (the user count) does make it possible to retrieve all information within 60 requests.
4. GraphQL as the solution. I explored GraphQL and the fact that Github does provide GraphQL style access to the users data.
5. Prior to coding the scraping program in Python, I wanted to get some experience on the GraphQL queries in the context of this project (finetune the query on a limited set of data (I restricted the outputs to just 10 records to check the query, the node structures etc. - the Github Explorer fortunately has intellisense and that helps a lot in getting the field names right), so to speak - similar to using a SQL tool for databases). I used the Github provided tool "Github GraphQL Explorer" for that. Please note that this need that you've a personal access token (PAT). Once the GraphQL query is tested (on a very limited set of outputs), I transferred the query over to the Python program.
6. Ready for testing (first iteration): Realized that query ran fine, but while printing the results onto the console, the execution stopped halfway. 
